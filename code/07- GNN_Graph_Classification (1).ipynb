{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch-geometric\n",
    "#!pip install -q torch-sparse\n",
    "#!pip install torch-scatter\n",
    "#!pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install pytorch-ignite\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Login to Weights and biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wandb login a42d563f399d54cd749774995c194c7dc83395eb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from queries import*\n",
    "from utilities import*\n",
    "from scipy.spatial import Delaunay\n",
    "from numpy import linalg\n",
    "from sklearn import preprocessing\n",
    "import copy\n",
    "import torch\n",
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import SAGEConv\n",
    "from torch_geometric.nn import GATConv\n",
    "import torch.nn as nn\n",
    "from torch_geometric.nn import global_mean_pool, global_max_pool\n",
    "import torch\n",
    "from torch_geometric.data import Dataset\n",
    "from torch_geometric.data import DataLoader\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import MessagePassing\n",
    "from torch_geometric.utils import add_self_loops, degree\n",
    "from torch_geometric.nn import GATConv\n",
    "\n",
    "import wandb\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Directory = Path(Path.cwd()).parent\n",
    "Directory_path = os.path.join(Directory, 'plots', 'GNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.size'] = '12'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(project=\"GNN-DEEP-MELO\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def labeling(string):\n",
    "    Patients_CR = ['1C', '1Y', '1N', '1J', '2Y', '1O', '1P','3H','1T','3M','3D','2A','2F','1R', '4A', '4C', '4I', '4Q'] \n",
    "    if str(string)[0:2] in Patients_CR:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def edges_to_tensor(edge_df):\n",
    "    \n",
    "    patch_number_1 = edge_df.patch_number_1.values\n",
    "    patch_number_2 = edge_df.patch_number_2.values\n",
    "    \n",
    "    edges_index = torch.tensor([patch_number_1,patch_number_2])\n",
    "    \n",
    "    return edges_index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_and_pick(liste, number):\n",
    "    \n",
    "    \n",
    "    random.shuffle(liste)\n",
    "    train_indicies = liste[:number]\n",
    "    test_indicies =liste[number:]\n",
    "    random.seed()\n",
    "    print('Train_indices:', train_indicies)\n",
    "    print('Test_indices:', test_indicies)\n",
    "    \n",
    "    return train_indicies, test_indicies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=0, alpha=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha,(float,int)): self.alpha = torch.Tensor([alpha,1-alpha])\n",
    "        if isinstance(alpha,list): self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.dim()>2:\n",
    "            input = input.view(input.size(0),input.size(1),-1)  # N,C,H,W => N,C,H*W\n",
    "            input = input.transpose(1,2)    # N,C,H*W => N,H*W,C\n",
    "            input = input.contiguous().view(-1,input.size(2))   # N,H*W,C => N*H*W,C\n",
    "            print(input.shape())\n",
    "            target = target.view(-1,1)\n",
    "\n",
    "        logpt = F.log_softmax(input)\n",
    "        logpt = logpt.gather(1,target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = Variable(logpt.data.exp())\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type()!=input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            at = self.alpha.gather(0,target.data.view(-1))\n",
    "            logpt = logpt * Variable(at)\n",
    "\n",
    "        loss = -1 * (1-pt)**self.gamma * logpt\n",
    "        if self.size_average: return loss.mean()\n",
    "        else: return loss.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "base_directory = Path(Path.cwd()).parent\n",
    "Directory_path = os.path.join(base_directory,'data','output')\n",
    "files = os.listdir(Directory_path)\n",
    "\n",
    "labels = []\n",
    "patients_name = []\n",
    "labels_df = pd.DataFrame({},columns =['patient','label'])\n",
    "\n",
    "for file in files[:]:\n",
    "    \n",
    "    array = file.split('_')\n",
    "    if len(array)==2:\n",
    "        condition = array[0]\n",
    "        PATIENT_ID = array[1]\n",
    "    else:\n",
    "        condition = ''\n",
    "        \n",
    "    \n",
    "    if condition == 'DEEPMEL':\n",
    "        \n",
    "        labels.append(labeling(PATIENT_ID))\n",
    "        patients_name.append(PATIENT_ID)\n",
    "        \n",
    "labels_df['patient'] = patients_name\n",
    "labels_df['label'] = labels\n",
    "\n",
    "labels_df = labels_df.drop_duplicates(['patient']).reset_index().drop(['index'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(labels_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_csv_file = labels_df.to_csv(os.path.join(Directory_path, 'patients_labels.csv'), index= False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining Graph Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_graph_objects(label_csv_file, patch_features_root_dir, transform_edges = edges_to_tensor):\n",
    "    \n",
    "    patient_labels_df = pd.read_csv(label_csv_file)\n",
    "    \n",
    "    patient_names = patient_labels_df.patient.values\n",
    "    \n",
    "    data_list = []\n",
    "    \n",
    "    for index,patient_name in enumerate(patient_names):\n",
    "        \n",
    "        base_directory = Path(Path.cwd()).parent\n",
    "        patient_folder = os.path.join(base_directory,'data','output', 'DEEPMEL_' + patient_name)\n",
    "        \n",
    "        patch_features_df = pd.read_csv(os.path.join(patient_folder, 'patch_data_1.csv'))\n",
    "        patch_features_df = patch_features_df.sort_values(['patch_number']).set_index(['patch_number'])\n",
    "        patch_features_df = patch_features_df.fillna(0)\n",
    "\n",
    "        patch_edges_df = pd.read_csv(os.path.join(patient_folder, 'patch_edge_data.csv'))\n",
    "        patch_edges_df.dropna(how='all', axis=1, inplace=True)\n",
    "        #patch_features_df.drop(['patch', 'cell_x_position', 'cell_y_position'])\n",
    "        #print(patch_features_df)\n",
    "\n",
    "        x = torch.from_numpy(patch_features_df.drop(['patch', 'cell_x_position', 'cell_y_position'], axis=1).values)\n",
    "        #x = torch.reshape(x, (1, x.size()))\n",
    "        #print(x.size())\n",
    "        \n",
    "        edge_index = transform_edges(patch_edges_df)\n",
    "        \n",
    "        y = torch.tensor(patient_labels_df[patient_labels_df.patient == patient_name ].label.values)\n",
    "        #print(y.size())\n",
    "        \n",
    "        data = Data(y=y, x = x, edge_index= edge_index)\n",
    "        \n",
    "        data_list.append(data)\n",
    "        #print(data)\n",
    "    \n",
    "    #print(data_list)\n",
    "\n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients_dataset = get_graph_objects(Directory_path + '\\\\patients_labels.csv',Directory_path )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training testing partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_class_1 = list(labels_df[labels_df.label == 1].index)\n",
    "index_class_0 = list(labels_df[labels_df.label == 0].index)\n",
    "\n",
    "train_indicies_class_1 , test_indicies_class_1 =  shuffle_and_pick(index_class_1, 14)\n",
    "train_indicies_class_0 , test_indicies_class_0 =  shuffle_and_pick(index_class_0, 47)\n",
    "\n",
    "train_indicies = train_indicies_class_1 + train_indicies_class_0\n",
    "test_indicies = test_indicies_class_1 + test_indicies_class_0 \n",
    "\n",
    "train_dataset = [patients_dataset[i] for i in train_indicies]\n",
    "test_dataset = [patients_dataset[i] for i in test_indicies]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(patients_dataset[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in train_loader:\n",
    "    batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, data in enumerate(train_loader):\n",
    "    print(f'Step {step + 1}:')\n",
    "    print('=======')\n",
    "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "    print(data)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for step, data in enumerate(test_loader):\n",
    "    print(f'Step {step + 1}:')\n",
    "    print('=======')\n",
    "    print(f'Number of graphs in the current batch: {data.num_graphs}')\n",
    "    print(data)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GCN Two layers graph convolutional neural networks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The graph convolutional operator from the \"Semi-supervised Classification with Graph Convolutional Networks\" paper\n",
    "$$\n",
    "\\mathbf{X}^{\\prime}=\\hat{\\mathbf{D}}^{-1 / 2} \\hat{\\mathbf{A}} \\hat{\\mathbf{D}}^{-1 / 2} \\mathbf{X} \\boldsymbol{\\Theta}\n",
    "$$\n",
    "where $\\hat{\\mathbf{A}}=\\mathbf{A}+\\mathbf{I}$ denotes the adjacency matrix with inserted self-loops and $\\hat{D}_{i i}=\\sum_{j=0} \\hat{A}_{i j}$ its diagonal degree matrix. The adjacency matrix can include other values than 1 representing\n",
    "edge weights via the optional edge_weight tensor.\n",
    "Its node-wise formulation is given by:\n",
    "$$\n",
    "\\mathbf{x}_{i}^{\\prime}=\\boldsymbol{\\Theta} \\sum_{j \\in \\mathcal{N}(v) \\cup\\{i\\}} \\frac{e_{j, i}}{\\sqrt{\\hat{d}_{j} \\hat{d}_{i}}} \\mathbf{x}_{j}\n",
    "$$\n",
    "with $\\hat{d}_{i}=1+\\sum_{j \\in \\mathcal{N}(i)} e_{j, i}$, where $e_{j, i}$ denotes the edge weight from source node $\\mathrm{j}$ to target node $i$ (default: $1.0$ )\n",
    "PARAMETERS:\n",
    "- in_channels (int) - Size of each input sample.\n",
    "- out_channels (int) - Size of each output sample.\n",
    "- improved (bool, optional) - If set to True, the layer computes $\\hat{\\mathbf{A}}$ as $\\mathbf{A}+2 \\mathbf{I}$. (default: False)\n",
    "- cached (bool, optional) - If set to True, the layer will cache the computation of $\\hat{\\mathbf{D}}^{-1 / 2} \\hat{\\mathbf{A}} \\hat{\\mathbf{D}}^{-1 / 2}$ on first execution, and will use the cached version for further executions. This parameter should only be\n",
    "set to True in transductive learning scenarios. (default: False)\n",
    "- add_self_loops (bool, optional) - If set to Fatse, will not add self-loops to the input graph. (default: True)\n",
    "\n",
    "- normalize (bool, optional) – Whether to add self-loops and compute symmetric normalization coefficients on the fly. (default: True)\n",
    "\n",
    "- bias (bool, optional) – If set to False, the layer will not learn an additive bias. (default: True)\n",
    "\n",
    "**kwargs (optional) – Additional arguments of torch_geometric.nn.conv.MessagePassing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Linear\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self,in_channels, hidden_channels, n_classes, P):\n",
    "        super(GCN, self).__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(in_channels, hidden_channels, add_self_loops = True)\n",
    "        self.conv2 = GCNConv(hidden_channels, hidden_channels, add_self_loops = True)\n",
    "        self.lin = Linear(hidden_channels, n_classes)\n",
    "\n",
    "    def forward(self, data, P=0.8):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        # 1. Obtain node embeddings \n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "\n",
    "        # 2. Readout layer\n",
    "        x = global_mean_pool(x, batch)  # [batch_size, hidden_channels]\n",
    "\n",
    "        # 3. Apply a final classifier\n",
    "        x = F.dropout(x, p=P, training=self.training)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model.train()\n",
    "    LOSS = []\n",
    "    for data in train_loader:  # Iterate in batches over the training dataset.\n",
    "        out = model(data)\n",
    "        #print(data.y)\n",
    "        #print(out)# Perform a single forward pass.\n",
    "        loss = criterion(out, data.y)# Compute the loss.\n",
    "        LOSS.append(np.double(loss.detach().numpy()))\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "    return LOSS\n",
    "\n",
    "\n",
    "def valid():\n",
    "\n",
    "    model.eval()\n",
    "    TEST_LOSS = []\n",
    "    \n",
    "    for data in test_loader:  # Iterate in batches over the training dataset.\n",
    "        out = model(data)  # Perform a single forward pass.\n",
    "        loss = criterion(out, data.y)# Compute the loss.\n",
    "        TEST_LOSS.append(np.double(loss.detach().numpy()))\n",
    "       \n",
    "        \n",
    "    return TEST_LOSS\n",
    "    \n",
    "        \n",
    "        \n",
    "        \n",
    "\n",
    "def test(loader):\n",
    "    model.eval()\n",
    "   \n",
    "    TP = 0\n",
    "    FP = 0\n",
    "    FN = 0\n",
    "    TN = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for data in loader:  # Iterate in batches over the training/test dataset.\n",
    "        out = model(data)  \n",
    "        pred = out.argmax(dim=1) # Use the class with highest probability.\n",
    "        correct += int((pred == data.y).sum())\n",
    "        print('pred:',pred)\n",
    "        print('true:',data.y)\n",
    "        print(confusion_matrix(np.array(data.y),np.array(pred)).ravel())\n",
    "        tn, fp, fn, tp = confusion_matrix(np.array(data.y),np.array(pred), labels=[0,1]).ravel()\n",
    "        if len(confusion_matrix(np.array(data.y),np.array(pred), labels=[0,1]).ravel()) >1 :\n",
    "            TP += tp\n",
    "            FP += fp\n",
    "            FN += fn\n",
    "            TN += tn\n",
    "        else : \n",
    "            if (np.array(data.y)[0]==0):\n",
    "                TN += len(confusion_matrix(np.array(data.y),np.array(pred), labels=[0,1]).ravel()) \n",
    "                \n",
    "            else :\n",
    "                TP += len(confusion_matrix(np.array(data.y),np.array(pred), labels=[0,1]).ravel())\n",
    "            \n",
    "       \n",
    "        \n",
    "    #print(TP, FP, FN)\n",
    "    precision_1 = TP /(TP+FP)\n",
    "    recall_1 = TP / (TP+FN)\n",
    "    precision_0 = TN /(TN+FN)\n",
    "    recall_0 = TN / (TN+FP)\n",
    "    return correct / len(loader.dataset), precision_1, recall_1,precision_0, recall_0# Derive ratio of correct predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  GCN : Fixing Model HyperParameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10\n",
    "\n",
    "\n",
    "index_class_1 = list(labels_df[labels_df.label == 1].index)\n",
    "index_class_0 = list(labels_df[labels_df.label == 0].index)\n",
    "\n",
    "train_indicies_class_1 , test_indicies_class_1 =  shuffle_and_pick(index_class_1, 14)\n",
    "train_indicies_class_0 , test_indicies_class_0 =  shuffle_and_pick(index_class_0, 47)\n",
    "\n",
    "train_indicies = train_indicies_class_1 + train_indicies_class_0\n",
    "test_indicies = test_indicies_class_1 + test_indicies_class_0 \n",
    "\n",
    "train_dataset = [patients_dataset[i] for i in train_indicies] \n",
    "test_dataset = [patients_dataset[i] for i in test_indicies]\n",
    "#print(train_dataset)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False) \n",
    "wandb.init(project='DEEP-MLO')\n",
    "\n",
    "Learning_rate = 0.01\n",
    "IN_CHANNELS = 64\n",
    "HIDDEN_CHANNELS = BATCH_SIZE\n",
    "N_CLASSES = 2\n",
    "DROPOUT = 0\n",
    "CLASS_1_WEIGHT = 3\n",
    "ALPHA = 5\n",
    "GAMMA = 5 \n",
    "\n",
    "\n",
    "\n",
    "model = GCN(IN_CHANNELS,HIDDEN_CHANNELS,N_CLASSES, DROPOUT).double()\n",
    "\n",
    "opt_1 = torch.optim.Adam(model.parameters(), lr=Learning_rate)\n",
    "\n",
    "opt_2 = torch.optim.SGD(model.parameters(), lr=Learning_rate, momentum = 0.9)\n",
    "\n",
    "optimizer = opt_1\n",
    "\n",
    "\n",
    "WEIGHTS = torch.FloatTensor([1,CLASS_1_WEIGHT]).double()\n",
    "criterion = torch.nn.CrossEntropyLoss(weight = WEIGHTS)\n",
    "#criterion = FocalLoss(GAMMA,ALPHA)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "LOSS_epoch_GCN = []\n",
    "TOTAL_LOSS = []\n",
    "TEST_epoch = []\n",
    "recall_class_0 =[]\n",
    "precision_class_0 =[]\n",
    "recall_class_1 =[]\n",
    "precision_class_1 =[]\n",
    "\n",
    "Train_accuracy = []\n",
    "Test_accuracy = []\n",
    "\n",
    "\n",
    "for epoch in range(1, 201):\n",
    "    loss = train()\n",
    "    test_loss = valid()\n",
    "    print(loss)\n",
    "    TOTAL_LOSS = TOTAL_LOSS + loss\n",
    "    LOSS_epoch_GCN.append(np.mean(loss))\n",
    "    TEST_epoch.append(np.mean(test_loss))\n",
    "    train_acc = test(train_loader)[0]\n",
    "    print('test')\n",
    "    test_acc = test(test_loader)[0]\n",
    "    recall_1 = test(test_loader)[2]\n",
    "    precision_1 = test(test_loader)[1]\n",
    "    precision_0 = test(test_loader)[3]\n",
    "    recall_0 = test(test_loader)[4]\n",
    "    \n",
    "    recall_class_0.append(recall_0)\n",
    "    recall_class_1.append(recall_1)\n",
    "    \n",
    "    precision_class_0.append(precision_0)\n",
    "    precision_class_1.append(precision_1)\n",
    "   \n",
    "    Train_accuracy.append(train_acc)\n",
    "    Test_accuracy.append(test_acc)\n",
    "\n",
    "\n",
    "\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}, precision_1: {precision_1:.4f}, recall_1: {recall_1:.4f},precision_0: {precision_0:.4f},recall_0: {recall_0:.4f}')\n",
    "    wandb.log({\"loss\": np.mean(loss),\"Train accuracy\": train_acc, \"Test acuuracy\":test_acc, 'precision_1':precision_1,'recall_1':recall_1,'precision_0': precision_0,'recall_0': recall_0})\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "n = len(TOTAL_LOSS)\n",
    "plt.plot(range(n),TOTAL_LOSS[:]   )\n",
    "PLOT_DIRECTORY = os.path.join(base_directory,'plots')\n",
    "PLOT_PATH = os.path.join(PLOT_DIRECTORY, 'DEEPMEL_' + PATIENT_ID)\n",
    "plt.xlabel('steps')\n",
    "plt.ylabel('LOSS')\n",
    "plt.grid()\n",
    "plt.title('Training loss' )\n",
    "file_name = r'Training_loss_model_GCN_(lr='+str(Learning_rate)+'),(DROPOUT='+str(DROPOUT)+'),(CLASS_1_WEIGHT='+str(CLASS_1_WEIGHT)+').png'\n",
    "plt.savefig(os.path.join(PLOT_DIRECTORY ,file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "n = len(recall_class_0)\n",
    "x_axis = range(n)\n",
    "plt.plot(x_axis,recall_class_0, label = 'Recall 0'  )\n",
    "plt.plot(x_axis,recall_class_1, label = 'Recall 1'  )\n",
    "plt.plot(x_axis,precision_class_0, label = 'Precision 0'  )\n",
    "plt.plot(x_axis,precision_class_1, label = 'Precision 1'  )\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Metrics')\n",
    "plt.title('Testing metrics')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(PLOT_DIRECTORY + 'Test_recall_precision_model GCN (lr ='+str(Learning_rate)+') , (DROPOUT='+str(DROPOUT)+'), (CLASS_1_WEIGHT='+str(CLASS_1_WEIGHT)+').png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "\n",
    "plt.plot(Train_accuracy[:],label= 'Training Accuracy')\n",
    "plt.plot(Test_accuracy[:], label= 'Test Accuracy' )\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.title( 'Training- Test accuracy' )\n",
    "plt.savefig(PLOT_DIRECTORY + 'Train -Test Accuracy _model GCN_ (lr ='+str(Learning_rate)+') , (DROPOUT='+str(DROPOUT)+'), (CLASS_1_WEIGHT='+str(CLASS_1_WEIGHT)+').png')\n",
    "#plt.savefig(PLOT_DIRECTORY + 'Train-Test Accuracy: model GCN Best.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "plt.plot(LOSS_epoch_GCN[:],label= 'Training loss'   )\n",
    "plt.plot(TEST_epoch[:], label= 'validation loss' )\n",
    "\n",
    "\n",
    "plt.xlabel('steps')\n",
    "plt.ylabel('LOSS')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.title( 'Training and validation loss GCN model  ' )\n",
    "plt.savefig(PLOT_DIRECTORY + 'Training_vs_Validation_loss model GCN (lr ='+str(Learning_rate)+') , (DROPOUT='+str(DROPOUT)+'), (CLASS_1_WEIGHT='+str(CLASS_1_WEIGHT)+').png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weights and Biases : Sweeps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sweep_config = {\n",
    "  \"name\" : \"GCN_sweep\",\n",
    "  \"method\" : \"grid\",\n",
    "   \"metric\":{\n",
    "  \"name\" : \"loss\"},\n",
    "  \n",
    "  \"parameters\" : {\n",
    "    \"epochs\" : {\n",
    "      \"values\" : [200]\n",
    "    },\n",
    "    \"learning_rate\" :{\n",
    "      \"min\": 0.0001,\n",
    "      \"max\": 0.1\n",
    "    },\n",
    "      \"batch_size\" : {\n",
    "          \"distribution\" :'int_uniform',\n",
    "          \"min\":1,\n",
    "          \"max\":40\n",
    "      },\n",
    "      \n",
    "      \"class_1_weight\": {\n",
    "          \n",
    "          \"min\":1,\n",
    "          \"max\":20\n",
    "      }\n",
    "      \n",
    "      \n",
    "  }\n",
    "}\n",
    "\n",
    "#sweep_id = wandb.sweep(sweep_config,project='DEEP-MLO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def train_1():\n",
    "#    with wandb.init() as run:\n",
    "#        config = wandb.config\n",
    "#        BATCH_SIZE = config[\"batch_size\"]\n",
    "#        LR = config[\"learning_rate\"]\n",
    "#        DROPOUT = 0\n",
    "#        CLASS_1_WEIGHT = config[\"class_1_weight\"]\n",
    "#        WEIGHTS = torch.FloatTensor([1,CLASS_1_WEIGHT]).double()\n",
    "        \n",
    "#        train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "#        test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True)    \n",
    "        \n",
    "        \n",
    "#        model = GCN(IN_CHANNELS,HIDDEN_CHANNELS,N_CLASSES, DROPOUT).double()\n",
    "        \n",
    "#        optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "#        criterion = torch.nn.CrossEntropyLoss(WEIGHTS).double()\n",
    "    \n",
    "#        for epoch in range(1, 201):\n",
    "#            loss = train()\n",
    "#            avg_loss = np.mean(loss)\n",
    "#            wandb.log({\"loss\": np.mean(loss)})\n",
    "        \n",
    "\n",
    "#count = 10 # number of runs to execute\n",
    "#wandb.agent(sweep_id = 'x8o354ox', function=train_1,count=count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10-fold Cross validation  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FOLDS = 10\n",
    "Learning_rate = 0.01\n",
    "IN_CHANNELS = 64\n",
    "HIDDEN_CHANNELS = BATCH_SIZE\n",
    "N_CLASSES = 2\n",
    "DROPOUT = 0\n",
    "CLASS_1_WEIGHT = 3\n",
    "ALPHA = 5\n",
    "GAMMA = 5\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "\n",
    "index_class_1 = list(labels_df[labels_df.label == 1].index)\n",
    "index_class_0 = list(labels_df[labels_df.label == 0].index)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "WEIGHTS = torch.FloatTensor([1,CLASS_1_WEIGHT]).double()\n",
    "criterion = torch.nn.CrossEntropyLoss(weight = WEIGHTS)\n",
    "\n",
    "\n",
    "TRAIN_ACC_BOXPLOT = []\n",
    "TEST_ACC_BOXPLOT = []\n",
    "RECALL_1_BOXPLOT = []\n",
    "RECALL_0_BOXPLOT = []\n",
    "PRECISION_1_BOXPLOT = []\n",
    "PRECISION_0_BOXPLOT = []\n",
    "CONVERGED_TRAINING_LOSS = []\n",
    "CONVERGED_VALID_LOSS = []\n",
    "\n",
    "\n",
    "for step in range(N_FOLDS):\n",
    "    \n",
    "    print('Fold : ',step)\n",
    " \n",
    "    model = GCN(IN_CHANNELS,HIDDEN_CHANNELS,N_CLASSES, DROPOUT).double()\n",
    "    criterion = torch.nn.CrossEntropyLoss(weight = WEIGHTS)\n",
    "    opt_1 = torch.optim.Adam(model.parameters(), lr=Learning_rate)\n",
    "    opt_2 = torch.optim.SGD(model.parameters(), lr=Learning_rate, momentum = 0.9)\n",
    "    optimizer = opt_1\n",
    "    \n",
    "    train_indicies_class_1 , test_indicies_class_1 =  shuffle_and_pick(index_class_1, 14)\n",
    "    train_indicies_class_0 , test_indicies_class_0 =  shuffle_and_pick(index_class_0, 47)\n",
    "    \n",
    "\n",
    "    train_indicies = train_indicies_class_1 + train_indicies_class_0\n",
    "    test_indicies = test_indicies_class_1 + test_indicies_class_0 \n",
    "\n",
    "    train_dataset = [patients_dataset[i] for i in train_indicies] \n",
    "    test_dataset = [patients_dataset[i] for i in test_indicies]\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False) \n",
    "    \n",
    "  \n",
    "    \n",
    "    LOSS_epoch = []\n",
    "    TOTAL_LOSS = []\n",
    "    TEST_epoch = []\n",
    "    recall_class_0 =[]\n",
    "    precision_class_0 =[]\n",
    "    recall_class_1 =[]\n",
    "    precision_class_1 =[]\n",
    "\n",
    "    Train_accuracy = []\n",
    "    Test_accuracy = []\n",
    "\n",
    "\n",
    "    for epoch in range(1, 201):\n",
    "        loss = train()\n",
    "        test_loss = valid()\n",
    "        #print(loss)\n",
    "        TOTAL_LOSS = TOTAL_LOSS + loss\n",
    "        LOSS_epoch.append(np.mean(loss))\n",
    "        TEST_epoch.append(np.mean(test_loss))\n",
    "        train_acc = test(train_loader)[0]\n",
    "        #print('test')\n",
    "        test_acc = test(test_loader)[0]\n",
    "        recall_1 = test(test_loader)[2]\n",
    "        precision_1 = test(test_loader)[1]\n",
    "        precision_0 = test(test_loader)[3]\n",
    "        recall_0 = test(test_loader)[4]\n",
    "    \n",
    "        recall_class_0.append(recall_0)\n",
    "        recall_class_1.append(recall_1)\n",
    "    \n",
    "        precision_class_0.append(precision_0)\n",
    "        precision_class_1.append(precision_1)\n",
    "   \n",
    "        Train_accuracy.append(train_acc)\n",
    "        Test_accuracy.append(test_acc)\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}, precision_1: {precision_1:.4f}, recall_1: {recall_1:.4f},precision_0: {precision_0:.4f},recall_0: {recall_0:.4f}')\n",
    "        \n",
    "    TRAIN_ACC_BOXPLOT.append(train_acc)\n",
    "    TEST_ACC_BOXPLOT.append(test_acc)\n",
    "    RECALL_1_BOXPLOT.append(recall_1)\n",
    "    RECALL_0_BOXPLOT.append(recall_0)\n",
    "    PRECISION_1_BOXPLOT.append(precision_1)\n",
    "    PRECISION_0_BOXPLOT.append(precision_0)\n",
    "    CONVERGED_TRAINING_LOSS.append(np.mean(loss))\n",
    "    CONVERGED_VALID_LOSS.append(np.mean(test_loss))  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = pd.DataFrame({},columns =['training_loss', 'validation_loss'])\n",
    "losses['training_loss'] = CONVERGED_TRAINING_LOSS\n",
    "losses['validation_loss'] = CONVERGED_VALID_LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = pd.DataFrame({},columns =['train accuracy', 'test accuracy', 'recall_1', 'precision_1', 'recall_0','precision_0'])\n",
    "metrics['train accuracy'] = TRAIN_ACC_BOXPLOT\n",
    "metrics['test accuracy'] = TEST_ACC_BOXPLOT\n",
    "metrics['recall_1'] = RECALL_1_BOXPLOT\n",
    "metrics['recall_0'] = RECALL_0_BOXPLOT\n",
    "metrics['precision_1'] = PRECISION_1_BOXPLOT\n",
    "metrics['precision_0'] = PRECISION_0_BOXPLOT\n",
    "\n",
    "base_directory = Path(Path.cwd()).parent\n",
    "OUTPUT = os.path.join(base_directory,'data','output')\n",
    "\n",
    "METRICS_PATH = os.path.join(OUTPUT, 'metrics')\n",
    "\n",
    "metrics.to_csv(METRICS_PATH + '\\\\GCN.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "sns.boxplot(data=metrics)\n",
    "plt.xlabel('METRICS')\n",
    "plt.ylabel('VALUES')\n",
    "plt.grid()\n",
    "plt.title( 'Boxplots of different metrics for GCN model' )\n",
    "plt.savefig(PLOT_DIRECTORY + 'BOXPLOTS GCN: (lr ='+str(Learning_rate)+') , (DROPOUT='+str(DROPOUT)+'), (CLASS_1_WEIGHT='+str(CLASS_1_WEIGHT)+').png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAT \n",
    "\n",
    "The graph attentional operator from the \"Graph Attention Networks\" paper\n",
    "$$\n",
    "\\mathbf{x}_{i}^{\\prime}=\\alpha_{i, i} \\boldsymbol{\\Theta} \\mathbf{x}_{i}+\\sum_{j \\in \\mathcal{N}(i)} \\alpha_{i, j} \\boldsymbol{\\Theta} \\mathbf{x}_{j}\n",
    "$$\n",
    "where the attention coefficients $\\alpha_{i, j}$ are computed as\n",
    "$$\n",
    "\\alpha_{i, j}=\\frac{\\exp \\left(\\operatorname{LeakyReLU}\\left(\\mathbf{a}^{\\top}\\left[\\boldsymbol{\\Theta} \\mathbf{x}_{i} \\| \\boldsymbol{\\Theta} \\mathbf{x}_{j}\\right]\\right)\\right)}{\\sum_{k \\in \\mathcal{N}(i) \\cup\\{i\\}} \\exp \\left(\\operatorname{LeakyReLU}\\left(\\mathbf{a}^{\\top}\\left[\\boldsymbol{\\Theta} \\mathbf{x}_{i} \\| \\boldsymbol{\\Theta} \\mathbf{x}_{k}\\right]\\right)\\right)}\n",
    "$$\n",
    "PARAMETERS:\\\n",
    "$\\quad$ - in_channels (int or tuple) - Size of each input sample. A tuple corresponds to the sizes of source and target dimensionalities.\n",
    "- out_channels (int) - Size of each output sample.\n",
    "- heads (int, optional) - Number of multi-head-attentions. (default: 1 )\n",
    "- concat (bool, optional) - If set to Fatse, the multi-head attentions are\n",
    "averaged instead of concatenated. (default: True )\n",
    "- negative_slope (float, optional) - LeakyReLU angle of the negative slope. (default: $0.2$ )\n",
    "- dropout (float, optional) - Dropout probability of the normalized attention coefficients which exposes each node to a stochastically sampled neighborhood during training. (default: 0 )\n",
    "- add_self_loops (bool, optional) - If set to False, will not add self-loops\n",
    "to the input graph. (default: True )\n",
    "- bias (bool, optional) - If set to False, the layer will not learn an additive\n",
    "bias. (default: True)\n",
    "- $^{* *}$ kwargs (optional) - Additional arguments of\n",
    "torch_geometric.nn. conv.MessagePassing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAT(nn.Module):\n",
    "    def __init__(self, hidden_channels, out_channels, first_heads, output_heads, dropout):\n",
    "        super(GAT, self).__init__()\n",
    "        self.gc1 = GATConv(64, hidden_channels,\n",
    "                           heads=first_heads, dropout=dropout)\n",
    "        \n",
    "        self.gc2 = GATConv(hidden_channels*first_heads, hidden_channels,\n",
    "                           heads=output_heads, dropout=dropout)\n",
    "        \n",
    "        self.dropout = dropout\n",
    "        self.lin = Linear(hidden_channels, 2)\n",
    "\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.gc1.reset_parameters()\n",
    "        self.gc2.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch= data.x, data.edge_index, data.batch\n",
    "        #x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.gc1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        #x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.gc2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10\n",
    "\n",
    "\n",
    "index_class_1 = list(labels_df[labels_df.label == 1].index)\n",
    "index_class_0 = list(labels_df[labels_df.label == 0].index)\n",
    "\n",
    "train_indicies_class_1 , test_indicies_class_1 =  shuffle_and_pick(index_class_1, 14)\n",
    "train_indicies_class_0 , test_indicies_class_0 =  shuffle_and_pick(index_class_0, 47)\n",
    "\n",
    "train_indicies = train_indicies_class_1 + train_indicies_class_0\n",
    "test_indicies = test_indicies_class_1 + test_indicies_class_0 \n",
    "\n",
    "train_dataset = [patients_dataset[i] for i in train_indicies] \n",
    "test_dataset = [patients_dataset[i] for i in test_indicies]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False) \n",
    "#wandb.init(project='oncology-project')\n",
    "\n",
    "Learning_rate = 0.01\n",
    "IN_CHANNELS = 64\n",
    "HIDDEN_CHANNELS = BATCH_SIZE\n",
    "N_CLASSES = 2\n",
    "DROPOUT = 0\n",
    "CLASS_1_WEIGHT = 3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = GAT( hidden_channels =BATCH_SIZE, out_channels =2,first_heads=8, output_heads=1, dropout=DROPOUT).double()\n",
    "\n",
    "opt_1 = torch.optim.Adam(model.parameters(), lr=Learning_rate)\n",
    "\n",
    "opt_2 = torch.optim.SGD(model.parameters(), lr=Learning_rate, momentum = 0.9)\n",
    "\n",
    "optimizer = opt_1\n",
    "\n",
    "\n",
    "WEIGHTS = torch.FloatTensor([1,CLASS_1_WEIGHT]).double()\n",
    "criterion = torch.nn.CrossEntropyLoss(weight = WEIGHTS)\n",
    "#criterion = FocalLoss(GAMMA,ALPHA)\n",
    "\n",
    "LOSS_epoch_GAT = []\n",
    "TOTAL_LOSS = []\n",
    "TEST_epoch = []\n",
    "recall_class_0 =[]\n",
    "precision_class_0 =[]\n",
    "recall_class_1 =[]\n",
    "precision_class_1 =[]\n",
    "\n",
    "Train_accuracy = []\n",
    "Test_accuracy = []\n",
    "\n",
    "\n",
    "for epoch in range(1, 201):\n",
    "    loss = train()\n",
    "    test_loss = valid()\n",
    "    print(loss)\n",
    "    TOTAL_LOSS = TOTAL_LOSS + loss\n",
    "    LOSS_epoch_GAT.append(np.mean(loss))\n",
    "    TEST_epoch.append(np.mean(test_loss))\n",
    "    train_acc = test(train_loader)[0]\n",
    "    print('test')\n",
    "    test_acc = test(test_loader)[0]\n",
    "    recall_1 = test(test_loader)[2]\n",
    "    precision_1 = test(test_loader)[1]\n",
    "    precision_0 = test(test_loader)[3]\n",
    "    recall_0 = test(test_loader)[4]\n",
    "    \n",
    "    recall_class_0.append(recall_0)\n",
    "    recall_class_1.append(recall_1)\n",
    "    \n",
    "    precision_class_0.append(precision_0)\n",
    "    precision_class_1.append(precision_1)\n",
    "   \n",
    "    Train_accuracy.append(train_acc)\n",
    "    Test_accuracy.append(test_acc)\n",
    "\n",
    "\n",
    "\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}, precision_1: {precision_1:.4f}, recall_1: {recall_1:.4f},precision_0: {precision_0:.4f},recall_0: {recall_0:.4f}')\n",
    "    #wandb.log({\"loss\": np.mean(loss),\"Train accuracy\": train_acc, \"Test acuuracy\":test_acc, 'precision_1':precision_1,'recall_1':recall_1,'precision_0': precision_0,'recall_0': recall_0})\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "n = len(recall_class_0)\n",
    "x_axis = range(n)\n",
    "plt.plot(x_axis,recall_class_0, label = 'Recall 0'  )\n",
    "plt.plot(x_axis,recall_class_1, label = 'Recall 1'  )\n",
    "plt.plot(x_axis,precision_class_0, label = 'Precision 0'  )\n",
    "plt.plot(x_axis,precision_class_1, label = 'Precision 1'  )\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Metrics')\n",
    "plt.title('Testing metrics: GAT MODEL')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(PLOT_DIRECTORY + 'Test_recall_precision model GAT (lr ='+str(Learning_rate)+') , (DROPOUT='+str(DROPOUT)+'), (CLASS_1_WEIGHT='+str(CLASS_1_WEIGHT)+').png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "n = len(LOSS_epoch_GAT)\n",
    "plt.plot(range(n),Train_accuracy[:],label= 'Training Accuracy'   )\n",
    "plt.plot(range(n),Test_accuracy[:], label= 'Test Accuracy' )\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.title( 'Training- Test accuracy: GAT Model' )\n",
    "plt.savefig(PLOT_DIRECTORY + 'Train -Test Accuracy model GAT (lr ='+str(Learning_rate)+') , (DROPOUT='+str(DROPOUT)+'), (CLASS_1_WEIGHT='+str(CLASS_1_WEIGHT)+').png')\n",
    "#plt.savefig(PLOT_DIRECTORY + 'Train-Test Accuracy: model GCN Best.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "n = len(LOSS_epoch_GAT)\n",
    "plt.plot(range(n),LOSS_epoch_GAT[:],label= 'Training loss'   )\n",
    "plt.plot(range(n),TEST_epoch[:], label= 'validation loss' )\n",
    "\n",
    "\n",
    "\n",
    "plt.xlabel('steps')\n",
    "plt.ylabel('LOSS')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.title( 'Training loss' )\n",
    "plt.savefig(PLOT_DIRECTORY + 'Training_vs_Validation_loss model GAT (lr ='+str(Learning_rate)+') , (DROPOUT='+str(DROPOUT)+'), (CLASS_1_WEIGHT='+str(CLASS_1_WEIGHT)+').png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAT CROSS VALIDATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FOLDS = 10\n",
    "Learning_rate = 0.01\n",
    "IN_CHANNELS = 64\n",
    "HIDDEN_CHANNELS = BATCH_SIZE\n",
    "N_CLASSES = 2\n",
    "DROPOUT = 0\n",
    "CLASS_1_WEIGHT = 3\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "\n",
    "index_class_1 = list(labels_df[labels_df.label == 1].index)\n",
    "index_class_0 = list(labels_df[labels_df.label == 0].index)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "WEIGHTS = torch.FloatTensor([1,CLASS_1_WEIGHT]).double()\n",
    "criterion = torch.nn.CrossEntropyLoss(weight = WEIGHTS)\n",
    "\n",
    "\n",
    "TRAIN_ACC_BOXPLOT = []\n",
    "TEST_ACC_BOXPLOT = []\n",
    "RECALL_1_BOXPLOT = []\n",
    "RECALL_0_BOXPLOT = []\n",
    "PRECISION_1_BOXPLOT = []\n",
    "PRECISION_0_BOXPLOT = []\n",
    "CONVERGED_TRAINING_LOSS = []\n",
    "CONVERGED_VALID_LOSS = []\n",
    "\n",
    "\n",
    "for step in range(N_FOLDS):\n",
    "    \n",
    "    print('Fold : ',step)\n",
    " \n",
    "    model = GAT( hidden_channels =BATCH_SIZE, out_channels =2,first_heads=8, output_heads=1, dropout=DROPOUT).double()\n",
    "    criterion = torch.nn.CrossEntropyLoss(weight = WEIGHTS)\n",
    "    opt_1 = torch.optim.Adam(model.parameters(), lr=Learning_rate)\n",
    "    opt_2 = torch.optim.SGD(model.parameters(), lr=Learning_rate, momentum = 0.9)\n",
    "    optimizer = opt_1\n",
    "    \n",
    "    train_indicies_class_1 , test_indicies_class_1 =  shuffle_and_pick(index_class_1, 14)\n",
    "    train_indicies_class_0 , test_indicies_class_0 =  shuffle_and_pick(index_class_0, 47)\n",
    "\n",
    "    train_indicies = train_indicies_class_1 + train_indicies_class_0\n",
    "    test_indicies = test_indicies_class_1 + test_indicies_class_0 \n",
    "\n",
    "    train_dataset = [patients_dataset[i] for i in train_indicies] \n",
    "    test_dataset = [patients_dataset[i] for i in test_indicies]\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False) \n",
    "    \n",
    "  \n",
    "    \n",
    "    LOSS_epoch = []\n",
    "    TOTAL_LOSS = []\n",
    "    TEST_epoch = []\n",
    "    recall_class_0 =[]\n",
    "    precision_class_0 =[]\n",
    "    recall_class_1 =[]\n",
    "    precision_class_1 =[]\n",
    "\n",
    "    Train_accuracy = []\n",
    "    Test_accuracy = []\n",
    "\n",
    "\n",
    "    for epoch in range(1, 201):\n",
    "        loss = train()\n",
    "        test_loss = valid()\n",
    "        #print(loss)\n",
    "        TOTAL_LOSS = TOTAL_LOSS + loss\n",
    "        LOSS_epoch.append(np.mean(loss))\n",
    "        TEST_epoch.append(np.mean(test_loss))\n",
    "        train_acc = test(train_loader)[0]\n",
    "        #print('test')\n",
    "        test_acc = test(test_loader)[0]\n",
    "        recall_1 = test(test_loader)[2]\n",
    "        precision_1 = test(test_loader)[1]\n",
    "        precision_0 = test(test_loader)[3]\n",
    "        recall_0 = test(test_loader)[4]\n",
    "    \n",
    "        recall_class_0.append(recall_0)\n",
    "        recall_class_1.append(recall_1)\n",
    "    \n",
    "        precision_class_0.append(precision_0)\n",
    "        precision_class_1.append(precision_1)\n",
    "   \n",
    "        Train_accuracy.append(train_acc)\n",
    "        Test_accuracy.append(test_acc)\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}, precision_1: {precision_1:.4f}, recall_1: {recall_1:.4f},precision_0: {precision_0:.4f},recall_0: {recall_0:.4f}')\n",
    "        \n",
    "    TRAIN_ACC_BOXPLOT.append(train_acc)\n",
    "    TEST_ACC_BOXPLOT.append(test_acc)\n",
    "    RECALL_1_BOXPLOT.append(recall_1)\n",
    "    RECALL_0_BOXPLOT.append(recall_0)\n",
    "    PRECISION_1_BOXPLOT.append(precision_1)\n",
    "    PRECISION_0_BOXPLOT.append(precision_0)\n",
    "    CONVERGED_TRAINING_LOSS.append(np.mean(loss))\n",
    "    CONVERGED_VALID_LOSS.append(np.mean(test_loss))  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_GAT = pd.DataFrame({},columns =['training_loss', 'validation_loss'])\n",
    "losses_GAT['training_loss'] = CONVERGED_TRAINING_LOSS\n",
    "losses_GAT['validation_loss'] = CONVERGED_VALID_LOSS\n",
    "\n",
    "\n",
    "metrics_GAT = pd.DataFrame({},columns =['train accuracy', 'test accuracy', 'recall_1', 'precision_1', 'recall_0','precision_0'])\n",
    "metrics_GAT['train accuracy'] = TRAIN_ACC_BOXPLOT\n",
    "metrics_GAT['test accuracy'] = TEST_ACC_BOXPLOT\n",
    "metrics_GAT['recall_1'] = RECALL_1_BOXPLOT\n",
    "metrics_GAT['recall_0'] = RECALL_0_BOXPLOT\n",
    "metrics_GAT['precision_1'] = PRECISION_1_BOXPLOT\n",
    "metrics_GAT['precision_0'] = PRECISION_0_BOXPLOT\n",
    "\n",
    "metrics_GAT.to_csv(METRICS_PATH + '\\\\GAT.csv', index=False)\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "sns.boxplot(data=metrics_GAT)\n",
    "plt.xlabel('METRICS')\n",
    "plt.ylabel('VALUES')\n",
    "plt.grid()\n",
    "plt.title( 'Boxplots of different metrics for GAT model' )\n",
    "plt.savefig(PLOT_DIRECTORY + 'BOXPLOTS METRICS GAT (lr ='+str(Learning_rate)+') , (DROPOUT='+str(DROPOUT)+'), (CLASS_1_WEIGHT='+str(CLASS_1_WEIGHT)+').png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "sns.boxplot(data=losses_GAT)\n",
    "plt.xlabel('Losses')\n",
    "plt.ylabel('VALUES')\n",
    "plt.grid()\n",
    "plt.title( 'Boxplots of different metrics for GAT model' )\n",
    "plt.savefig(PLOT_DIRECTORY + 'BOXPLOTS LOSSES GAT (lr ='+str(Learning_rate)+') , (DROPOUT='+str(DROPOUT)+'), (CLASS_1_WEIGHT='+str(CLASS_1_WEIGHT)+').png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SGC : Simple graph convolution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simple graph convolutional operator from the \"Simplifying Graph Convolutional Networks\"\n",
    "paper\n",
    "$$\n",
    "\\mathbf{X}^{\\prime}=\\left(\\hat{\\mathbf{D}}^{-1 / 2} \\hat{\\mathbf{A}} \\hat{\\mathbf{D}}^{-1 / 2}\\right)^{K} \\mathbf{X} \\boldsymbol{\\Theta}\n",
    "$$\n",
    "where $\\hat{\\mathbf{A}}=\\mathbf{A}+\\mathbf{I}$ denotes the adjacency matrix with inserted self-loops and $\\hat{D}_{i i}=\\sum_{j=0} \\hat{A}_{i j}$ its diagonal degree matrix. The adjacency matrix can include other values than 1 representing edge weights via the optional edge_weight tensor.\n",
    "PARAMETERS:\n",
    "- in_channels (int) - Size of each input sample.\n",
    "- out_channels (int) - Size of each output sample.\n",
    "- $\\mathrm{K}$ (int, optional) - Number of hops $K$. (default: 1 )\n",
    "- cached (bool, optional) - If set to True, the layer will cache the computation of $\\left(\\hat{\\mathbf{D}}^{-1 / 2} \\hat{\\mathbf{A}} \\hat{\\mathbf{D}}^{-1 / 2}\\right)^{K} \\mathbf{X}$ on first execution, and will use the cached version for further executions. This parameter should only be set to True in transductive learning scenarios. (default: False)\n",
    "- add_self_loops (bool, optional) - If set to Fatse, will not add self-loops to the input graph. (default: True )\n",
    "- bias (bool, optional) - If set to False, the layer will not learn an additive bias. (default: True)\n",
    "- $^{* *}$ kwargs (optional) - Additional arguments of torch_geometric.nn. conv.MessagePassing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import SGConv\n",
    "\n",
    "\n",
    "class SGC(nn.Module):\n",
    "    def __init__(self,hidden_channels, K):\n",
    "        super(SGC, self).__init__()\n",
    "        self.gc1 = SGConv(64, hidden_channels, K=K,add_self_loops=True)\n",
    "        self.gc2 = SGConv(hidden_channels, hidden_channels, K=K,add_self_loops=True)\n",
    "\n",
    "        self.lin = Linear(hidden_channels, 2)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.gc1.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = self.gc1(x, edge_index)\n",
    "        x= F.relu(x)\n",
    "        x = self.gc2(x, edge_index)\n",
    "        x= F.relu(x)\n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.lin(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10\n",
    "\n",
    "\n",
    "index_class_1 = list(labels_df[labels_df.label == 1].index)\n",
    "index_class_0 = list(labels_df[labels_df.label == 0].index)\n",
    "\n",
    "train_indicies_class_1 , test_indicies_class_1 =  shuffle_and_pick(index_class_1, 14)\n",
    "train_indicies_class_0 , test_indicies_class_0 =  shuffle_and_pick(index_class_0, 47)\n",
    "\n",
    "train_indicies = train_indicies_class_1 + train_indicies_class_0\n",
    "test_indicies = test_indicies_class_1 + test_indicies_class_0 \n",
    "\n",
    "train_dataset = [patients_dataset[i] for i in train_indicies] \n",
    "test_dataset = [patients_dataset[i] for i in test_indicies]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False) \n",
    "#wandb.init(project='oncology-project')\n",
    "\n",
    "Learning_rate = 0.01\n",
    "IN_CHANNELS = 64\n",
    "HIDDEN_CHANNELS = BATCH_SIZE\n",
    "N_CLASSES = 2\n",
    "DROPOUT = 0\n",
    "CLASS_1_WEIGHT = 3\n",
    "K = 2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = SGC(hidden_channels=BATCH_SIZE,K=K).double()\n",
    "\n",
    "opt_1 = torch.optim.Adam(model.parameters(), lr=Learning_rate)\n",
    "\n",
    "opt_2 = torch.optim.SGD(model.parameters(), lr=Learning_rate, momentum = 0.9)\n",
    "\n",
    "optimizer = opt_1\n",
    "\n",
    "\n",
    "WEIGHTS = torch.FloatTensor([1,CLASS_1_WEIGHT]).double()\n",
    "criterion = torch.nn.CrossEntropyLoss(weight = WEIGHTS)\n",
    "#criterion = FocalLoss(GAMMA,ALPHA)\n",
    "\n",
    "LOSS_epoch = []\n",
    "TOTAL_LOSS = []\n",
    "TEST_epoch = []\n",
    "recall_class_0 =[]\n",
    "precision_class_0 =[]\n",
    "recall_class_1 =[]\n",
    "precision_class_1 =[]\n",
    "\n",
    "Train_accuracy = []\n",
    "Test_accuracy = []\n",
    "\n",
    "\n",
    "for epoch in range(1, 201):\n",
    "    loss = train()\n",
    "    test_loss = valid()\n",
    "    print(loss)\n",
    "    TOTAL_LOSS = TOTAL_LOSS + loss\n",
    "    LOSS_epoch.append(np.mean(loss))\n",
    "    TEST_epoch.append(np.mean(test_loss))\n",
    "    train_acc = test(train_loader)[0]\n",
    "    print('test')\n",
    "    test_acc = test(test_loader)[0]\n",
    "    recall_1 = test(test_loader)[2]\n",
    "    precision_1 = test(test_loader)[1]\n",
    "    precision_0 = test(test_loader)[3]\n",
    "    recall_0 = test(test_loader)[4]\n",
    "    \n",
    "    recall_class_0.append(recall_0)\n",
    "    recall_class_1.append(recall_1)\n",
    "    \n",
    "    precision_class_0.append(precision_0)\n",
    "    precision_class_1.append(precision_1)\n",
    "   \n",
    "    Train_accuracy.append(train_acc)\n",
    "    Test_accuracy.append(test_acc)\n",
    "\n",
    "\n",
    "\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}, precision_1: {precision_1:.4f}, recall_1: {recall_1:.4f},precision_0: {precision_0:.4f},recall_0: {recall_0:.4f}')\n",
    "    #wandb.log({\"loss\": np.mean(loss),\"Train accuracy\": train_acc, \"Test acuuracy\":test_acc, 'precision_1':precision_1,'recall_1':recall_1,'precision_0': precision_0,'recall_0': recall_0})\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "n = len(recall_class_0)\n",
    "x_axis = range(n)\n",
    "plt.plot(x_axis,recall_class_0, label = 'Recall 0'  )\n",
    "plt.plot(x_axis,recall_class_1, label = 'Recall 1'  )\n",
    "plt.plot(x_axis,precision_class_0, label = 'Precision 0'  )\n",
    "plt.plot(x_axis,precision_class_1, label = 'Precision 1'  )\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Metrics')\n",
    "plt.title('Testing metrics: SGC MODEL')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(PLOT_DIRECTORY + 'Test_recall_precision model SGC (lr ='+str(Learning_rate)+') , (DROPOUT='+str(DROPOUT)+'), (CLASS_1_WEIGHT='+str(CLASS_1_WEIGHT)+').png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "n = len(LOSS_epoch)\n",
    "plt.plot(range(n),Train_accuracy[:],label= 'Training Accuracy'   )\n",
    "plt.plot(range(n),Test_accuracy[:], label= 'Test Accuracy' )\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.title( 'Training- Test accuracy: SGC Model' )\n",
    "plt.savefig(PLOT_DIRECTORY + 'Train -Test Accuracy model SGC (lr ='+str(Learning_rate)+') , (DROPOUT='+str(DROPOUT)+'), (CLASS_1_WEIGHT='+str(CLASS_1_WEIGHT)+').png')\n",
    "#plt.savefig(PLOT_DIRECTORY + 'Train-Test Accuracy: model GCN Best.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "n = len(LOSS_epoch)\n",
    "plt.plot(range(n),LOSS_epoch[:],label= 'Training loss'   )\n",
    "plt.plot(range(n),TEST_epoch[:], label= 'validation loss' )\n",
    "plt.xlabel('steps')\n",
    "plt.ylabel('LOSS')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.title( 'Training_ Validation loss : SGC model' )\n",
    "plt.savefig(PLOT_DIRECTORY + 'Training_vs_Validation_loss model SGC (lr ='+str(Learning_rate)+') , (DROPOUT='+str(DROPOUT)+'), (CLASS_1_WEIGHT='+str(CLASS_1_WEIGHT)+').png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FOLDS = 9\n",
    "Learning_rate = 0.01\n",
    "IN_CHANNELS = 64\n",
    "HIDDEN_CHANNELS = BATCH_SIZE\n",
    "N_CLASSES = 2\n",
    "DROPOUT = 0\n",
    "CLASS_1_WEIGHT = 3\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "\n",
    "index_class_1 = list(labels_df[labels_df.label == 1].index)\n",
    "index_class_0 = list(labels_df[labels_df.label == 0].index)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "WEIGHTS = torch.FloatTensor([1,CLASS_1_WEIGHT]).double()\n",
    "criterion = torch.nn.CrossEntropyLoss(weight = WEIGHTS)\n",
    "\n",
    "\n",
    "TRAIN_ACC_BOXPLOT = []\n",
    "TEST_ACC_BOXPLOT = []\n",
    "RECALL_1_BOXPLOT = []\n",
    "RECALL_0_BOXPLOT = []\n",
    "PRECISION_1_BOXPLOT = []\n",
    "PRECISION_0_BOXPLOT = []\n",
    "CONVERGED_TRAINING_LOSS = []\n",
    "CONVERGED_VALID_LOSS = []\n",
    "\n",
    "\n",
    "for step in range(N_FOLDS):\n",
    "    \n",
    "    print('Fold : ',step)\n",
    " \n",
    "    model = SGC(hidden_channels=BATCH_SIZE,K=K).double()\n",
    "    criterion = torch.nn.CrossEntropyLoss(weight = WEIGHTS)\n",
    "    opt_1 = torch.optim.Adam(model.parameters(), lr=Learning_rate)\n",
    "    opt_2 = torch.optim.SGD(model.parameters(), lr=Learning_rate, momentum = 0.9)\n",
    "    optimizer = opt_1\n",
    "    \n",
    "    train_indicies_class_1 , test_indicies_class_1 =  shuffle_and_pick(index_class_1, 14)\n",
    "    train_indicies_class_0 , test_indicies_class_0 =  shuffle_and_pick(index_class_0, 47)\n",
    "\n",
    "    train_indicies = train_indicies_class_1 + train_indicies_class_0\n",
    "    test_indicies = test_indicies_class_1 + test_indicies_class_0 \n",
    "\n",
    "    train_dataset = [patients_dataset[i] for i in train_indicies] \n",
    "    test_dataset = [patients_dataset[i] for i in test_indicies]\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True) \n",
    "    \n",
    "  \n",
    "    \n",
    "    LOSS_epoch = []\n",
    "    TOTAL_LOSS = []\n",
    "    TEST_epoch = []\n",
    "    recall_class_0 =[]\n",
    "    precision_class_0 =[]\n",
    "    recall_class_1 =[]\n",
    "    precision_class_1 =[]\n",
    "\n",
    "    Train_accuracy = []\n",
    "    Test_accuracy = []\n",
    "\n",
    "\n",
    "    for epoch in range(1, 201):\n",
    "        loss = train()\n",
    "        test_loss = valid()\n",
    "        #print(loss)\n",
    "        TOTAL_LOSS = TOTAL_LOSS + loss\n",
    "        LOSS_epoch.append(np.mean(loss))\n",
    "        TEST_epoch.append(np.mean(test_loss))\n",
    "        train_acc = test(train_loader)[0]\n",
    "        #print('test')\n",
    "        test_acc = test(test_loader)[0]\n",
    "        recall_1 = test(test_loader)[2]\n",
    "        precision_1 = test(test_loader)[1]\n",
    "        precision_0 = test(test_loader)[3]\n",
    "        recall_0 = test(test_loader)[4]\n",
    "    \n",
    "        recall_class_0.append(recall_0)\n",
    "        recall_class_1.append(recall_1)\n",
    "    \n",
    "        precision_class_0.append(precision_0)\n",
    "        precision_class_1.append(precision_1)\n",
    "   \n",
    "        Train_accuracy.append(train_acc)\n",
    "        Test_accuracy.append(test_acc)\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}, precision_1: {precision_1:.4f}, recall_1: {recall_1:.4f},precision_0: {precision_0:.4f},recall_0: {recall_0:.4f}')\n",
    "        \n",
    "    TRAIN_ACC_BOXPLOT.append(train_acc)\n",
    "    TEST_ACC_BOXPLOT.append(test_acc)\n",
    "    RECALL_1_BOXPLOT.append(recall_1)\n",
    "    RECALL_0_BOXPLOT.append(recall_0)\n",
    "    PRECISION_1_BOXPLOT.append(precision_1)\n",
    "    PRECISION_0_BOXPLOT.append(precision_0)\n",
    "    CONVERGED_TRAINING_LOSS.append(np.mean(loss))\n",
    "    CONVERGED_VALID_LOSS.append(np.mean(test_loss)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_SGC = pd.DataFrame({},columns =['training_loss', 'validation_loss'])\n",
    "losses_SGC['training_loss'] = CONVERGED_TRAINING_LOSS\n",
    "losses_SGC['validation_loss'] = CONVERGED_VALID_LOSS\n",
    "\n",
    "\n",
    "metrics_SGC = pd.DataFrame({},columns =['train accuracy', 'test accuracy', 'recall_1', 'precision_1', 'recall_0','precision_0'])\n",
    "metrics_SGC['train accuracy'] = TRAIN_ACC_BOXPLOT\n",
    "metrics_SGC['test accuracy'] = TEST_ACC_BOXPLOT\n",
    "metrics_SGC['recall_1'] = RECALL_1_BOXPLOT\n",
    "metrics_SGC['recall_0'] = RECALL_0_BOXPLOT\n",
    "metrics_SGC['precision_1'] = PRECISION_1_BOXPLOT\n",
    "metrics_SGC['precision_0'] = PRECISION_0_BOXPLOT\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "sns.boxplot(data=metrics_SGC)\n",
    "plt.xlabel('METRICS')\n",
    "plt.ylabel('VALUES')\n",
    "plt.grid()\n",
    "plt.title( 'Boxplots of different metrics for SGC model' )\n",
    "plt.savefig(PLOT_DIRECTORY + 'BOXPLOTS METRICS SGC: (lr ='+str(Learning_rate)+') , (DROPOUT='+str(DROPOUT)+'), (CLASS_1_WEIGHT='+str(CLASS_1_WEIGHT)+').png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SAGEConv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\mathbf{x}_{i}^{\\prime}=\\mathbf{W}_{1} \\mathbf{x}_{i}+\\mathbf{W}_{2} \\cdot \\operatorname{mean}_{j \\in \\mathcal{N}(i)} \\mathbf{x}_{j}\n",
    "$$\n",
    "PARAMETERS:\n",
    "- in_channels (int or tuple) - Size of each input sample. A tuple corresponds to the sizes of source and target dimensionalities.\n",
    "- out_channels (int) - Size of each output sample.\n",
    "- normalize (bool, optional) - If set to True, output features will be $\\ell_{2}$ normalized, i.e., $\\frac{\\mathbf{x}_{i}^{\\prime}}{\\left\\|\\mathbf{x}_{i}^{\\prime}\\right\\|_{2}}$. (default: False)\n",
    "- root_weight (bool, optional) - If set to False, the layer will not add transformed root node features to the output. (default: True )\n",
    "- bias (bool, optional) - If set to False, the layer will not learn an additive bias. (default: True)\n",
    "$=^{* *} \\mathrm{kwargs}$ (optional) - Additional arguments of torch_geometric.nn. conv.MessagePassing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "\n",
    "class SAGE(nn.Module):\n",
    "    def __init__(self,hidden_channels,normalize = True):\n",
    "        super(SAGE, self).__init__()\n",
    "        self.gc1 = SAGEConv(64, hidden_channels,normalize=normalize)\n",
    "        self.gc2 = SAGEConv(hidden_channels, hidden_channels,normalize=normalize)\n",
    "        self.lin = Linear(hidden_channels, 2)\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.gc1.reset_parameters()\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "        x = self.gc1(x, edge_index)\n",
    "        x= F.relu(x)\n",
    "        x = self.gc2(x, edge_index)\n",
    "        x= F.relu(x)\n",
    "            \n",
    "        x = global_mean_pool(x, batch)\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10\n",
    "\n",
    "\n",
    "index_class_1 = list(labels_df[labels_df.label == 1].index)\n",
    "index_class_0 = list(labels_df[labels_df.label == 0].index)\n",
    "\n",
    "train_indicies_class_1 , test_indicies_class_1 =  shuffle_and_pick(index_class_1, 14)\n",
    "train_indicies_class_0 , test_indicies_class_0 =  shuffle_and_pick(index_class_0, 47)\n",
    "\n",
    "train_indicies = train_indicies_class_1 + train_indicies_class_0\n",
    "test_indicies = test_indicies_class_1 + test_indicies_class_0 \n",
    "\n",
    "train_dataset = [patients_dataset[i] for i in train_indicies] \n",
    "test_dataset = [patients_dataset[i] for i in test_indicies]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False) \n",
    "wandb.init(project='oncology-project')\n",
    "\n",
    "Learning_rate = 0.01\n",
    "IN_CHANNELS = 64\n",
    "HIDDEN_CHANNELS = BATCH_SIZE\n",
    "N_CLASSES = 2\n",
    "DROPOUT = 0\n",
    "CLASS_1_WEIGHT = 2\n",
    "K = 2\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "model = SAGE(hidden_channels=BATCH_SIZE).double()\n",
    "\n",
    "opt_1 = torch.optim.Adam(model.parameters(), lr=Learning_rate)\n",
    "\n",
    "opt_2 = torch.optim.SGD(model.parameters(), lr=Learning_rate, momentum = 0.9)\n",
    "\n",
    "optimizer = opt_1\n",
    "\n",
    "\n",
    "WEIGHTS = torch.FloatTensor([1,CLASS_1_WEIGHT]).double()\n",
    "criterion = torch.nn.CrossEntropyLoss(weight = WEIGHTS)\n",
    "#criterion = FocalLoss(GAMMA,ALPHA)\n",
    "\n",
    "LOSS_epoch_SAGE = []\n",
    "TOTAL_LOSS = []\n",
    "TEST_epoch = []\n",
    "recall_class_0 =[]\n",
    "precision_class_0 =[]\n",
    "recall_class_1 =[]\n",
    "precision_class_1 =[]\n",
    "\n",
    "Train_accuracy = []\n",
    "Test_accuracy = []\n",
    "\n",
    "\n",
    "for epoch in range(1, 201):\n",
    "    loss = train()\n",
    "    test_loss = valid()\n",
    "    print(loss)\n",
    "    TOTAL_LOSS = TOTAL_LOSS + loss\n",
    "    LOSS_epoch_SAGE.append(np.mean(loss))\n",
    "    TEST_epoch.append(np.mean(test_loss))\n",
    "    train_acc = test(train_loader)[0]\n",
    "    print('test')\n",
    "    test_acc = test(test_loader)[0]\n",
    "    recall_1 = test(test_loader)[2]\n",
    "    precision_1 = test(test_loader)[1]\n",
    "    precision_0 = test(test_loader)[3]\n",
    "    recall_0 = test(test_loader)[4]\n",
    "    \n",
    "    recall_class_0.append(recall_0)\n",
    "    recall_class_1.append(recall_1)\n",
    "    \n",
    "    precision_class_0.append(precision_0)\n",
    "    precision_class_1.append(precision_1)\n",
    "   \n",
    "    Train_accuracy.append(train_acc)\n",
    "    Test_accuracy.append(test_acc)\n",
    "\n",
    "\n",
    "\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}, precision_1: {precision_1:.4f}, recall_1: {recall_1:.4f},precision_0: {precision_0:.4f},recall_0: {recall_0:.4f}')\n",
    "    wandb.log({\"loss\": np.mean(loss),\"Train accuracy\": train_acc, \"Test acuuracy\":test_acc, 'precision_1':precision_1,'recall_1':recall_1,'precision_0': precision_0,'recall_0': recall_0})\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "n = len(recall_class_0)\n",
    "x_axis = range(n)\n",
    "plt.plot(x_axis,recall_class_0, label = 'Recall 0'  )\n",
    "plt.plot(x_axis,recall_class_1, label = 'Recall 1'  )\n",
    "plt.plot(x_axis,precision_class_0, label = 'Precision 0'  )\n",
    "plt.plot(x_axis,precision_class_1, label = 'Precision 1'  )\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Metrics')\n",
    "plt.title('Testing metrics: SAGE MODEL')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.savefig(PLOT_DIRECTORY + 'Test_recall_precision model SAGE (lr ='+str(Learning_rate)+') , (DROPOUT='+str(DROPOUT)+'), (CLASS_1_WEIGHT='+str(CLASS_1_WEIGHT)+').png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "n = len(LOSS_epoch_SAGE)\n",
    "plt.plot(range(n),Train_accuracy[:],label= 'Training Accuracy'   )\n",
    "plt.plot(range(n),Test_accuracy[:], label= 'Test Accuracy' )\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.title( 'Training- Test accuracy: SAGE Model' )\n",
    "plt.savefig(PLOT_DIRECTORY + 'Train -Test Accuracy model SAGE (lr ='+str(Learning_rate)+') , (DROPOUT='+str(DROPOUT)+'), (CLASS_1_WEIGHT='+str(CLASS_1_WEIGHT)+').png')\n",
    "#plt.savefig(PLOT_DIRECTORY + 'Train-Test Accuracy: model GCN Best.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "n = len(LOSS_epoch)\n",
    "plt.plot(range(n),LOSS_epoch_SAGE[:],label= 'Training loss'   )\n",
    "plt.plot(range(n),TEST_epoch[:], label= 'validation loss' )\n",
    "plt.xlabel('steps')\n",
    "plt.ylabel('LOSS')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.title( 'Training_ Validation loss : SAGE CONV model' )\n",
    "plt.savefig(PLOT_DIRECTORY + 'Training_vs_Validation_loss model SAGE (lr ='+str(Learning_rate)+') , (DROPOUT='+str(DROPOUT)+'), (CLASS_1_WEIGHT='+str(CLASS_1_WEIGHT)+').png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "a=15\n",
    "\n",
    "n = len(LOSS_epoch)-a\n",
    "plt.plot(range(n),LOSS_epoch_SAGE[a:],label= 'SAGE loss'   )\n",
    "plt.plot(range(n),LOSS_epoch_GAT[a:], label= 'GAT loss' )\n",
    "plt.plot(range(n),LOSS_epoch_GCN[a:], label= 'GCN loss' )\n",
    "\n",
    "plt.xlabel('steps')\n",
    "plt.ylabel('LOSS')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.title( 'Comparing different training losses : GAT vs SAGE vs GCN' )\n",
    "plt.savefig(PLOT_DIRECTORY + 'Comparaison Training losses GAT vs SAGE Vs GCN (lr ='+str(Learning_rate)+') , (DROPOUT='+str(DROPOUT)+'), (CLASS_1_WEIGHT='+str(CLASS_1_WEIGHT)+').png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_FOLDS = 10\n",
    "Learning_rate = 0.01\n",
    "IN_CHANNELS = 64\n",
    "HIDDEN_CHANNELS = BATCH_SIZE\n",
    "N_CLASSES = 2\n",
    "DROPOUT = 0\n",
    "CLASS_1_WEIGHT = 3\n",
    "BATCH_SIZE = 10\n",
    "\n",
    "\n",
    "index_class_1 = list(labels_df[labels_df.label == 1].index)\n",
    "index_class_0 = list(labels_df[labels_df.label == 0].index)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "WEIGHTS = torch.FloatTensor([1,CLASS_1_WEIGHT]).double()\n",
    "criterion = torch.nn.CrossEntropyLoss(weight = WEIGHTS)\n",
    "\n",
    "\n",
    "TRAIN_ACC_BOXPLOT = []\n",
    "TEST_ACC_BOXPLOT = []\n",
    "RECALL_1_BOXPLOT = []\n",
    "RECALL_0_BOXPLOT = []\n",
    "PRECISION_1_BOXPLOT = []\n",
    "PRECISION_0_BOXPLOT = []\n",
    "CONVERGED_TRAINING_LOSS = []\n",
    "CONVERGED_VALID_LOSS = []\n",
    "\n",
    "\n",
    "for step in range(N_FOLDS):\n",
    "    \n",
    "    print('Fold : ',step)\n",
    " \n",
    "    model = SAGE(hidden_channels=BATCH_SIZE).double()\n",
    "\n",
    "    criterion = torch.nn.CrossEntropyLoss(weight = WEIGHTS)\n",
    "    opt_1 = torch.optim.Adam(model.parameters(), lr=Learning_rate)\n",
    "    opt_2 = torch.optim.SGD(model.parameters(), lr=Learning_rate, momentum = 0.9)\n",
    "    optimizer = opt_1\n",
    "    \n",
    "    train_indicies_class_1 , test_indicies_class_1 =  shuffle_and_pick(index_class_1, 14)\n",
    "    train_indicies_class_0 , test_indicies_class_0 =  shuffle_and_pick(index_class_0, 47)\n",
    "\n",
    "    train_indicies = train_indicies_class_1 + train_indicies_class_0\n",
    "    test_indicies = test_indicies_class_1 + test_indicies_class_0 \n",
    "\n",
    "    train_dataset = [patients_dataset[i] for i in train_indicies] \n",
    "    test_dataset = [patients_dataset[i] for i in test_indicies]\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=True) \n",
    "    \n",
    "  \n",
    "    \n",
    "    LOSS_epoch = []\n",
    "    TOTAL_LOSS = []\n",
    "    TEST_epoch = []\n",
    "    recall_class_0 =[]\n",
    "    precision_class_0 =[]\n",
    "    recall_class_1 =[]\n",
    "    precision_class_1 =[]\n",
    "\n",
    "    Train_accuracy = []\n",
    "    Test_accuracy = []\n",
    "\n",
    "\n",
    "    for epoch in range(1, 201):\n",
    "        loss = train()\n",
    "        test_loss = valid()\n",
    "        #print(loss)\n",
    "        TOTAL_LOSS = TOTAL_LOSS + loss\n",
    "        LOSS_epoch.append(np.mean(loss))\n",
    "        TEST_epoch.append(np.mean(test_loss))\n",
    "        train_acc = test(train_loader)[0]\n",
    "        #print('test')\n",
    "        test_acc = test(test_loader)[0]\n",
    "        recall_1 = test(test_loader)[2]\n",
    "        precision_1 = test(test_loader)[1]\n",
    "        precision_0 = test(test_loader)[3]\n",
    "        recall_0 = test(test_loader)[4]\n",
    "    \n",
    "        recall_class_0.append(recall_0)\n",
    "        recall_class_1.append(recall_1)\n",
    "    \n",
    "        precision_class_0.append(precision_0)\n",
    "        precision_class_1.append(precision_1)\n",
    "   \n",
    "        Train_accuracy.append(train_acc)\n",
    "        Test_accuracy.append(test_acc)\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}, precision_1: {precision_1:.4f}, recall_1: {recall_1:.4f},precision_0: {precision_0:.4f},recall_0: {recall_0:.4f}')\n",
    "        \n",
    "    TRAIN_ACC_BOXPLOT.append(train_acc)\n",
    "    TEST_ACC_BOXPLOT.append(test_acc)\n",
    "    RECALL_1_BOXPLOT.append(recall_1)\n",
    "    RECALL_0_BOXPLOT.append(recall_0)\n",
    "    PRECISION_1_BOXPLOT.append(precision_1)\n",
    "    PRECISION_0_BOXPLOT.append(precision_0)\n",
    "    CONVERGED_TRAINING_LOSS.append(np.mean(loss))\n",
    "    CONVERGED_VALID_LOSS.append(np.mean(test_loss)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses_SAGE = pd.DataFrame({},columns =['training_loss', 'validation_loss'])\n",
    "losses_SAGE['training_loss'] = CONVERGED_TRAINING_LOSS\n",
    "losses_SAGE['validation_loss'] = CONVERGED_VALID_LOSS\n",
    "\n",
    "\n",
    "metrics_SAGE = pd.DataFrame({},columns =['train accuracy', 'test accuracy', 'recall_1', 'precision_1', 'recall_0','precision_0'])\n",
    "metrics_SAGE['train accuracy'] = TRAIN_ACC_BOXPLOT\n",
    "metrics_SAGE['test accuracy'] = TEST_ACC_BOXPLOT\n",
    "metrics_SAGE['recall_1'] = RECALL_1_BOXPLOT\n",
    "metrics_SAGE['recall_0'] = RECALL_0_BOXPLOT\n",
    "metrics_SAGE['precision_1'] = PRECISION_1_BOXPLOT\n",
    "metrics_SAGE['precision_0'] = PRECISION_0_BOXPLOT\n",
    "\n",
    "METRICS_PATH = os.path.join(OUTPUT, 'metrics')\n",
    "\n",
    "metrics_SAGE.to_csv(METRICS_PATH + '\\\\SAGE.csv', index=False)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(15,7))\n",
    "sns.boxplot(data=metrics_SAGE)\n",
    "plt.xlabel('METRICS')\n",
    "plt.ylabel('VALUES')\n",
    "plt.grid()\n",
    "plt.title( 'Boxplots of different metrics for SAGE model' )\n",
    "plt.savefig(PLOT_DIRECTORY + 'BOXPLOTS METRICS SAGE (lr ='+str(Learning_rate)+') , (DROPOUT='+str(DROPOUT)+'), (CLASS_1_WEIGHT='+str(CLASS_1_WEIGHT)+').png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing loss function on the same split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10\n",
    "\n",
    "\n",
    "index_class_1 = list(labels_df[labels_df.label == 1].index)\n",
    "index_class_0 = list(labels_df[labels_df.label == 0].index)\n",
    "\n",
    "train_indicies_class_1 , test_indicies_class_1 =  shuffle_and_pick(index_class_1, 14)\n",
    "train_indicies_class_0 , test_indicies_class_0 =  shuffle_and_pick(index_class_0, 47)\n",
    "\n",
    "train_indicies = train_indicies_class_1 + train_indicies_class_0\n",
    "test_indicies = test_indicies_class_1 + test_indicies_class_0 \n",
    "\n",
    "train_dataset = [patients_dataset[i] for i in train_indicies] \n",
    "test_dataset = [patients_dataset[i] for i in test_indicies]\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False) \n",
    "#wandb.init(project='oncology-project')\n",
    "\n",
    "Learning_rate = 0.01\n",
    "IN_CHANNELS = 64\n",
    "HIDDEN_CHANNELS = BATCH_SIZE\n",
    "N_CLASSES = 2\n",
    "DROPOUT = 0\n",
    "CLASS_1_WEIGHT = 3\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "WEIGHTS = torch.FloatTensor([1,CLASS_1_WEIGHT]).double()\n",
    "criterion = torch.nn.CrossEntropyLoss(weight = WEIGHTS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN(IN_CHANNELS,HIDDEN_CHANNELS,N_CLASSES, DROPOUT).double()\n",
    "opt_1 = torch.optim.Adam(model.parameters(), lr=Learning_rate)\n",
    "\n",
    "opt_2 = torch.optim.SGD(model.parameters(), lr=Learning_rate, momentum = 0.9)\n",
    "\n",
    "optimizer = opt_1\n",
    "LOSS_epoch_GCN = []\n",
    "TOTAL_LOSS = []\n",
    "TEST_epoch_GCN = []\n",
    "recall_class_0 =[]\n",
    "precision_class_0 =[]\n",
    "recall_class_1 =[]\n",
    "precision_class_1 =[]\n",
    "\n",
    "Train_accuracy_GCN = []\n",
    "Test_accuracy_GCN = []\n",
    "\n",
    "\n",
    "for epoch in range(1, 201):\n",
    "    loss = train()\n",
    "    test_loss = valid()\n",
    "    print(loss)\n",
    "    TOTAL_LOSS = TOTAL_LOSS + loss\n",
    "    LOSS_epoch_GCN.append(np.mean(loss))\n",
    "    TEST_epoch_GCN.append(np.mean(test_loss))\n",
    "    train_acc = test(train_loader)[0]\n",
    "    print('test')\n",
    "    test_acc = test(test_loader)[0]\n",
    "    recall_1 = test(test_loader)[2]\n",
    "    precision_1 = test(test_loader)[1]\n",
    "    precision_0 = test(test_loader)[3]\n",
    "    recall_0 = test(test_loader)[4]\n",
    "    \n",
    "    recall_class_0.append(recall_0)\n",
    "    recall_class_1.append(recall_1)\n",
    "    \n",
    "    precision_class_0.append(precision_0)\n",
    "    precision_class_1.append(precision_1)\n",
    "   \n",
    "    Train_accuracy_GCN.append(train_acc)\n",
    "    Test_accuracy_GCN.append(test_acc)\n",
    "\n",
    "\n",
    "\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}, precision_1: {precision_1:.4f}, recall_1: {recall_1:.4f},precision_0: {precision_0:.4f},recall_0: {recall_0:.4f}')\n",
    "    wandb.log({\"loss\": np.mean(loss),\"Train accuracy\": train_acc, \"Test acuuracy\":test_acc, 'precision_1':precision_1,'recall_1':recall_1,'precision_0': precision_0,'recall_0': recall_0})\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "n = len(LOSS_epoch_GCN)\n",
    "plt.plot(range(n),Train_accuracy_GCN[:],label= 'Training Accuracy'   )\n",
    "plt.plot(range(n),Test_accuracy_GCN[:], label= 'Test Accuracy' )\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.title( 'Training- Test accuracy: GCN Model' )\n",
    "plt.savefig(PLOT_DIRECTORY + 'Train -Test Accuracy model GCN (lr ='+str(Learning_rate)+') , (DROPOUT='+str(DROPOUT)+'), (CLASS_1_WEIGHT='+str(CLASS_1_WEIGHT)+').png')\n",
    "#plt.savefig(PLOT_DIRECTORY + 'Train-Test Accuracy: model GCN Best.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "n = len(LOSS_epoch_GCN)\n",
    "plt.plot(range(n),LOSS_epoch_GCN[:],label= 'Training loss'   )\n",
    "plt.plot(range(n),TEST_epoch_GCN[:], label= 'validation loss' )\n",
    "plt.xlabel('steps')\n",
    "plt.ylabel('LOSS')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.title( 'Training_ Validation loss : GCN model' )\n",
    "plt.savefig(PLOT_DIRECTORY + 'Training_vs_Validation_loss model GCN (lr ='+str(Learning_rate)+') , (DROPOUT='+str(DROPOUT)+'), (CLASS_1_WEIGHT='+str(CLASS_1_WEIGHT)+').png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GAT( hidden_channels =BATCH_SIZE, out_channels =2,first_heads=8, output_heads=1, dropout=DROPOUT).double()\n",
    "\n",
    "opt_1 = torch.optim.Adam(model.parameters(), lr=Learning_rate)\n",
    "\n",
    "opt_2 = torch.optim.SGD(model.parameters(), lr=Learning_rate, momentum = 0.9)\n",
    "\n",
    "optimizer = opt_1\n",
    "\n",
    "\n",
    "LOSS_epoch_GAT = []\n",
    "TOTAL_LOSS = []\n",
    "TEST_epoch_GAT = []\n",
    "recall_class_0 =[]\n",
    "precision_class_0 =[]\n",
    "recall_class_1 =[]\n",
    "precision_class_1 =[]\n",
    "\n",
    "Train_accuracy_GAT = []\n",
    "Test_accuracy_GAT = []\n",
    "\n",
    "\n",
    "for epoch in range(1, 201):\n",
    "    loss = train()\n",
    "    test_loss_GAT = valid()\n",
    "    print(loss)\n",
    "    TOTAL_LOSS = TOTAL_LOSS + loss\n",
    "    LOSS_epoch_GAT.append(np.mean(loss))\n",
    "    TEST_epoch_GAT.append(np.mean(test_loss))\n",
    "    train_acc = test(train_loader)[0]\n",
    "    print('test')\n",
    "    test_acc = test(test_loader)[0]\n",
    "    recall_1 = test(test_loader)[2]\n",
    "    precision_1 = test(test_loader)[1]\n",
    "    precision_0 = test(test_loader)[3]\n",
    "    recall_0 = test(test_loader)[4]\n",
    "    \n",
    "    recall_class_0.append(recall_0)\n",
    "    recall_class_1.append(recall_1)\n",
    "    \n",
    "    precision_class_0.append(precision_0)\n",
    "    precision_class_1.append(precision_1)\n",
    "   \n",
    "    Train_accuracy_GAT.append(train_acc)\n",
    "    Test_accuracy_GAT.append(test_acc)\n",
    "\n",
    "\n",
    "\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}, precision_1: {precision_1:.4f}, recall_1: {recall_1:.4f},precision_0: {precision_0:.4f},recall_0: {recall_0:.4f}')\n",
    "    wandb.log({\"loss\": np.mean(loss),\"Train accuracy\": train_acc, \"Test acuuracy\":test_acc, 'precision_1':precision_1,'recall_1':recall_1,'precision_0': precision_0,'recall_0': recall_0})\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "n = len(LOSS_epoch_GAT)\n",
    "plt.plot(range(n),Train_accuracy_GAT[:],label= 'Training Accuracy'   )\n",
    "plt.plot(range(n),Test_accuracy_GAT[:], label= 'Test Accuracy' )\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.title( 'Training- Test accuracy: GAT Model' )\n",
    "plt.savefig(PLOT_DIRECTORY + 'Train -Test Accuracy model GAT (lr ='+str(Learning_rate)+') , (DROPOUT='+str(DROPOUT)+'), (CLASS_1_WEIGHT='+str(CLASS_1_WEIGHT)+').png')\n",
    "#plt.savefig(PLOT_DIRECTORY + 'Train-Test Accuracy: model GCN Best.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "n = len(LOSS_epoch_GAT)\n",
    "plt.plot(range(n),LOSS_epoch_GAT[:],label= 'Training loss'   )\n",
    "plt.plot(range(n),TEST_epoch_GAT[:], label= 'validation loss' )\n",
    "plt.xlabel('steps')\n",
    "plt.ylabel('LOSS')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.title( 'Training_ Validation loss : GAT model' )\n",
    "plt.savefig(PLOT_DIRECTORY + 'Training_vs_Validation_loss model GAT (lr ='+str(Learning_rate)+') , (DROPOUT='+str(DROPOUT)+'), (CLASS_1_WEIGHT='+str(CLASS_1_WEIGHT)+').png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SAGE(hidden_channels=BATCH_SIZE).double()\n",
    "\n",
    "opt_1 = torch.optim.Adam(model.parameters(), lr=Learning_rate)\n",
    "\n",
    "opt_2 = torch.optim.SGD(model.parameters(), lr=Learning_rate, momentum = 0.9)\n",
    "\n",
    "optimizer = opt_1\n",
    "\n",
    "LOSS_epoch_SAGE = []\n",
    "TOTAL_LOSS = []\n",
    "TEST_epoch_SAGE = []\n",
    "recall_class_0 =[]\n",
    "precision_class_0 =[]\n",
    "recall_class_1 =[]\n",
    "precision_class_1 =[]\n",
    "\n",
    "Train_accuracy_SAGE = []\n",
    "Test_accuracy_SAGE= []\n",
    "\n",
    "\n",
    "for epoch in range(1, 201):\n",
    "    loss = train()\n",
    "    test_loss = valid()\n",
    "    print(loss)\n",
    "    TOTAL_LOSS = TOTAL_LOSS + loss\n",
    "    LOSS_epoch_SAGE.append(np.mean(loss))\n",
    "    TEST_epoch_SAGE.append(np.mean(test_loss))\n",
    "    train_acc = test(train_loader)[0]\n",
    "    print('test')\n",
    "    test_acc = test(test_loader)[0]\n",
    "    recall_1 = test(test_loader)[2]\n",
    "    precision_1 = test(test_loader)[1]\n",
    "    precision_0 = test(test_loader)[3]\n",
    "    recall_0 = test(test_loader)[4]\n",
    "    \n",
    "    recall_class_0.append(recall_0)\n",
    "    recall_class_1.append(recall_1)\n",
    "    \n",
    "    precision_class_0.append(precision_0)\n",
    "    precision_class_1.append(precision_1)\n",
    "   \n",
    "    Train_accuracy_SAGE.append(train_acc)\n",
    "    Test_accuracy_SAGE.append(test_acc)\n",
    "\n",
    "\n",
    "\n",
    "    print(f'Epoch: {epoch:03d}, Train Acc: {train_acc:.4f}, Test Acc: {test_acc:.4f}, precision_1: {precision_1:.4f}, recall_1: {recall_1:.4f},precision_0: {precision_0:.4f},recall_0: {recall_0:.4f}')\n",
    "    wandb.log({\"loss\": np.mean(loss),\"Train accuracy\": train_acc, \"Test acuuracy\":test_acc, 'precision_1':precision_1,'recall_1':recall_1,'precision_0': precision_0,'recall_0': recall_0})\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "n = len(LOSS_epoch_SAGE)\n",
    "plt.plot(range(n),Train_accuracy_SAGE[:],label= 'Training Accuracy'   )\n",
    "plt.plot(range(n),Test_accuracy_SAGE[:], label= 'Test Accuracy' )\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.title( 'Training- Test accuracy: SAGE Model' )\n",
    "plt.savefig(PLOT_DIRECTORY + 'Train -Test Accuracy model SAGE (lr ='+str(Learning_rate)+') , (DROPOUT='+str(DROPOUT)+'), (CLASS_1_WEIGHT='+str(CLASS_1_WEIGHT)+').png')\n",
    "#plt.savefig(PLOT_DIRECTORY + 'Train-Test Accuracy: model GCN Best.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "n = len(LOSS_epoch_GCN)\n",
    "plt.plot(range(n),LOSS_epoch_GCN[:],label= 'Training loss'   )\n",
    "plt.plot(range(n),TEST_epoch_GCN[:], label= 'validation loss' )\n",
    "plt.xlabel('steps')\n",
    "plt.ylabel('LOSS')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.title( 'Training_ Validation loss : GCN model' )\n",
    "plt.savefig(PLOT_DIRECTORY + 'Training_vs_Validation_loss model GCN (lr ='+str(Learning_rate)+') , (DROPOUT='+str(DROPOUT)+'), (CLASS_1_WEIGHT='+str(CLASS_1_WEIGHT)+').png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "a=100\n",
    "n = len(LOSS_epoch_GAT)\n",
    "x = np.linspace(a,n,n-a)\n",
    "\n",
    "\n",
    "plt.plot(x,LOSS_epoch_SAGE[a:],label= 'SAGE loss')\n",
    "plt.plot(x,LOSS_epoch_GAT[a:], label= 'GAT loss' )\n",
    "plt.plot(x,LOSS_epoch_GCN[a:], label= 'GCN loss' )\n",
    "\n",
    "plt.xlabel('steps')\n",
    "plt.ylabel('LOSS')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.title( 'Comparing different training losses : GAT vs SAGE vs GCN' )\n",
    "plt.savefig(PLOT_DIRECTORY + 'Comparaison Training losses GAT vs SAGE Vs GCN: (lr ='+str(Learning_rate)+') , (DROPOUT='+str(DROPOUT)+'), (CLASS_1_WEIGHT='+str(CLASS_1_WEIGHT)+')'+str(a)+'.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "a=100\n",
    "n = len(LOSS_epoch_GAT)\n",
    "x = np.linspace(a,n,n-a)\n",
    "\n",
    "plt.plot(x,LOSS_epoch_SAGE[a:],label= 'SAGE loss'   )\n",
    "plt.plot(x,LOSS_epoch_GAT[a:], label= 'GAT loss' )\n",
    "\n",
    "plt.xlabel('steps')\n",
    "plt.ylabel('LOSS')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.title( 'Comparing different training losses : GAT vs SAGE ' )\n",
    "plt.savefig(PLOT_DIRECTORY + 'Comparaison Training losses GAT vs SAGE: (lr ='+str(Learning_rate)+') , (DROPOUT='+str(DROPOUT)+'), (CLASS_1_WEIGHT='+str(CLASS_1_WEIGHT)+')'+str(a)+'.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,7))\n",
    "n = len(LOSS_epoch_GCN)\n",
    "plt.plot(range(n),Train_accuracy_GCN[:],label= 'Training Accuracy'   )\n",
    "plt.plot(range(n),Test_accuracy_GCN[:], label= 'Test Accuracy' )\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.title( 'Training- Test accuracy: GCN Model' )\n",
    "plt.savefig(PLOT_DIRECTORY + 'Train-Test Accuracy model GCN  (lr ='+str(Learning_rate)+') , (DROPOUT='+str(DROPOUT)+'), (CLASS_1_WEIGHT='+str(CLASS_1_WEIGHT)+').png')\n",
    "#plt.savefig(PLOT_DIRECTORY + 'Train-Test Accuracy: model GCN Best.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
